{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7817ac4-88fc-4404-b82f-4e6958e7ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92e06584-35fe-4878-9c1a-9126f75f208a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import datetime\n",
    "import random\n",
    "import string\n",
    "from dateutil import parser\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d2e75476-ab95-4a88-a798-89d353a5520d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "24b3df20-01b6-4e35-8a14-d335a7fa7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('atlas-chat.json', 'r') as f:\n",
    "  data = json.load(f)\n",
    "data = data['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ccd6873e-9610-4c54-bd71-8dc7f55807ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e33cc047-1013-484e-b9af-4de539a44dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57473c97-6be6-45b1-ab3a-a593f1e58c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c72f31-7d1b-4b71-b264-9117462a8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # PERSON - People, including fictional.\n",
    "    # NORP - Nationalities or religious or political groups.\n",
    "    # FAC - Buildings, airports, highways, bridges, etc.\n",
    "    # ORG - Companies, agencies, institutions, etc.\n",
    "    # GPE - Countries, cities, states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "776561c3-72f5-48f5-871d-54333009b0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nerLookupStr = '''PERSON:      People, including fictional.\n",
    "NORP:        Nationalities or religious or political groups.\n",
    "FAC:         Buildings, airports, highways, bridges, etc.\n",
    "ORG:         Companies, agencies, institutions, etc.\n",
    "GPE:         Countries, cities, states.\n",
    "LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "WORK_OF_ART: Titles of books, songs, etc.\n",
    "LAW:         Named documents made into laws.\n",
    "LANGUAGE:    Any named language.\n",
    "DATE:        Absolute or relative dates or periods.\n",
    "TIME:        Times smaller than a day.\n",
    "PERCENT:     Percentage, including ”%“.\n",
    "MONEY:       Monetary values, including unit.\n",
    "QUANTITY:    Measurements, as of weight or distance.\n",
    "ORDINAL:     “first”, “second”, etc.\n",
    "CARDINAL:    Numerals that do not fall under another type.'''\n",
    "nerLookup = dict()\n",
    "for line in nerLookupStr.split('\\n'):\n",
    "    items = [a.strip() for a in line.split(':')]\n",
    "    nerLookup[items[0].lower()] = items[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "00eeab73-cfaa-409d-a1fa-76a88109ef1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'person': 'People, including fictional.',\n",
       " 'norp': 'Nationalities or religious or political groups.',\n",
       " 'fac': 'Buildings, airports, highways, bridges, etc.',\n",
       " 'org': 'Companies, agencies, institutions, etc.',\n",
       " 'gpe': 'Countries, cities, states.',\n",
       " 'loc': 'Non-GPE locations, mountain ranges, bodies of water.',\n",
       " 'product': 'Objects, vehicles, foods, etc. (Not services.)',\n",
       " 'event': 'Named hurricanes, battles, wars, sports events, etc.',\n",
       " 'work_of_art': 'Titles of books, songs, etc.',\n",
       " 'law': 'Named documents made into laws.',\n",
       " 'language': 'Any named language.',\n",
       " 'date': 'Absolute or relative dates or periods.',\n",
       " 'time': 'Times smaller than a day.',\n",
       " 'percent': 'Percentage, including ”%“.',\n",
       " 'money': 'Monetary values, including unit.',\n",
       " 'quantity': 'Measurements, as of weight or distance.',\n",
       " 'ordinal': '“first”, “second”, etc.',\n",
       " 'cardinal': 'Numerals that do not fall under another type.'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nerLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "97a037e2-bf2e-4fb2-aecb-c8aac3cbe6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Define a function to extract named entities from a given text\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = []\n",
    "    for entity in doc.ents:\n",
    "        entities.append((entity.text, entity.label_))\n",
    "    return entities\n",
    "\n",
    "# Step 7: Define a function to generate entries with the desired format using spaCy named entities and word frequency\n",
    "def generate_entries(message_dataset):\n",
    "    entries = []\n",
    "\n",
    "    for week_start, week_end in tqdm(get_weeks_range(message_dataset)):\n",
    "        \n",
    "        week_entries = {\n",
    "            \"date\": week_start.isoformat(),\n",
    "            \"words\": {\n",
    "            }\n",
    "        }\n",
    "        ners = ['ORG', 'CARDINAL', 'DATE', 'GPE', 'PERSON', 'MONEY', 'PRODUCT', 'TIME', 'PERCENT', 'WORK_OF_ART', 'QUANTITY', 'NORP', 'LOC', 'EVENT', 'ORDINAL', 'FAC', 'LAW', 'LANGUAGE']\n",
    "\n",
    "        for ner in ners:\n",
    "            if ner.lower() in ['time','percent','quantity','date','cardinal','ordinal','money']:\n",
    "                continue\n",
    "\n",
    "            week_entries['words'].setdefault(nerLookup[ner.lower()], [])\n",
    "\n",
    "        word_counts = {}\n",
    "\n",
    "        for message in message_dataset:\n",
    "            message_date = message['timestamp'].date()\n",
    "\n",
    "            if week_start <= message_date <= week_end:\n",
    "                text = message['text']\n",
    "                entities = extract_named_entities(text)\n",
    "\n",
    "                for entity, label in entities:\n",
    "                    topic = label.lower()\n",
    "                    if topic in ['time','percent','quantity','date','cardinal','ordinal','money']:\n",
    "                        continue\n",
    "                    topic = nerLookup[topic]\n",
    "                    \n",
    "                    word = entity.lower()\n",
    "                    if word.isnumeric() or '#' in word:\n",
    "                        continue\n",
    "\n",
    "                    if word in word_counts:\n",
    "                        word_counts[word] += 1\n",
    "                    else:\n",
    "                        word_counts[word] = 1\n",
    "\n",
    "                    entry_id = f\"{word}_{topic}_{word_counts[word]}\"\n",
    "                    entry = {\n",
    "                        \"text\": word,\n",
    "                        \"frequency\": word_counts[word],\n",
    "                        \"topic\": topic,\n",
    "                        \"id\": entry_id\n",
    "                    }\n",
    "\n",
    "                    week_entries[\"words\"][topic].append(entry)\n",
    "\n",
    "        entries.append(week_entries)\n",
    "\n",
    "    return entries\n",
    "\n",
    "# Step 8: Define a function to get the range of weeks in the message dataset\n",
    "def get_weeks_range(message_dataset):\n",
    "    message_dates = [parser.parse(message['timestamp'].isoformat()).date() for message in message_dataset]\n",
    "    min_date = min(message_dates)\n",
    "    max_date = max(message_dates)\n",
    "\n",
    "    start_week = min_date.isocalendar()[1]\n",
    "    end_week = max_date.isocalendar()[1]\n",
    "\n",
    "    weeks_range = []\n",
    "\n",
    "    for week_number in range(start_week, end_week + 1):\n",
    "        week_start = min_date + datetime.timedelta(weeks=(week_number - start_week))\n",
    "        week_end = week_start + datetime.timedelta(days=6)\n",
    "        weeks_range.append((week_start, week_end))\n",
    "\n",
    "    return weeks_range\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5d396d27-e8e6-4669-b065-0dc52f67783e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'timestamp': datetime.datetime(2023, 4, 23, 16, 30, 54, 524000, tzinfo=tzutc()),\n",
       " 'text': \"you can do\\n```\\nimport nomic\\nnomic.login('yourtoken')\\n```\"}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 9: Provide your message dataset in the form of a list of dictionaries, where each dictionary represents a message and contains the timestamp and text content.\n",
    "\n",
    "# Sample message dataset\n",
    "message_dataset = []\n",
    "for d in data:\n",
    "    message_dataset.append({\n",
    "        'timestamp' : parser.parse(d['timestamp']),\n",
    "        'text' : d['content']\n",
    "    })\n",
    "\n",
    "random.choice(message_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d45b7441-d336-493a-8e35-aa20d8ee7fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 8/8 [00:03<00:00,  2.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entries = generate_entries(message_dataset)\n",
    "\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8f271c2e-3d1a-4307-9deb-0250b537959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to JSON:\n",
    "with open('atlas_data.json', 'w') as f:\n",
    "    json.dump(entries, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e8419-c8cc-4fd8-885b-8d8367ea5d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
